{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pyro\n",
    "import pyro.contrib.gp as gp\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch.autograd as autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = -5\n",
    "b = 0\n",
    "c = 0.5\n",
    "\n",
    "def test_function(X):\n",
    "    return a * torch.exp(-1.0 * torch.pow((X - b), 2) / (2*c*c))\n",
    "\n",
    "x_ = torch.linspace(-5, 5, 200)\n",
    "\n",
    "plt.plot(x_, test_function(x_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cnt = 3\n",
    "X_train = torch.tensor([x for x in np.random.uniform(low=-5, high=5, size=train_cnt)])\n",
    "Y_train = test_function(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpmodel = gp.models.GPRegression(X_train.T, Y_train, \n",
    "                                 gp.kernels.Matern52(input_dim=1, lengthscale=torch.ones(1), variance=torch.Tensor([150.0])), \n",
    "                                 noise=torch.tensor(0.1), \n",
    "                                 jitter=1.0e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimising GP's parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(gpmodel.parameters(), lr=0.005)\n",
    "loss_fn = pyro.infer.Trace_ELBO().differentiable_loss\n",
    "losses = []\n",
    "num_steps = 5000\n",
    "for i in range(num_steps):\n",
    "    optimizer.zero_grad()\n",
    "    loss = loss_fn(gpmodel.model, gpmodel.guide)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss.item())\n",
    "plt.semilogy(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expected improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_phi = lambda x: torch.exp(-x.pow(2)/2)/np.sqrt(2*np.pi)\n",
    "normal_Phi = lambda x: (1 + torch.erf(x / np.sqrt(2))) / 2\n",
    "  \n",
    "def expected_improvement(x):\n",
    "    \n",
    "    y_min = gpmodel.y.min()\n",
    "    \n",
    "    mu, variance = gpmodel(x, full_cov=False, noiseless=False)\n",
    "    \n",
    "    sigma = variance.sqrt()\n",
    "    \n",
    "    delta = y_min - mu\n",
    "    \n",
    "    EI = delta.clamp_min(0.0) + sigma*normal_phi(delta/sigma) - delta.abs()*normal_Phi(delta/sigma)\n",
    "    \n",
    "    return -EI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lower confidence bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_confidence_bound(x, kappa=2):\n",
    "    \n",
    "    mu, variance = gpmodel(x, full_cov=False, noiseless=False)\n",
    "    sigma = variance.sqrt()\n",
    "    \n",
    "    return mu - kappa * sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_, expected_improvement(x_).detach().numpy(), color=\"red\")\n",
    "plt.plot(x_, lower_confidence_bound(x_).detach().numpy(), color=\"blue\")\n",
    "plt.plot(x_, test_function(x_), color=\"black\")\n",
    "plt.plot(X_train, Y_train, \"*\", markersize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimising acquisition function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimise(acquisition_func, x_st):\n",
    "    \n",
    "    # unconstrained minimiser\n",
    "    minimizer = optim.LBFGS([x_st], lr=0.1)\n",
    "                        \n",
    "    def closure():\n",
    "        # clear gradients\n",
    "        minimizer.zero_grad()\n",
    "\n",
    "        y = acquisition_func(x_st)\n",
    "\n",
    "        autograd.backward(x_st, autograd.grad(y, x_st))\n",
    "\n",
    "        print(\"x_st\", x_st, y)\n",
    "\n",
    "        return y\n",
    "\n",
    "    minimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_st = torch.Tensor([0]).detach().requires_grad_(True)\n",
    "optimise(expected_improvement, x_st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_st = torch.Tensor([0.992]).detach().requires_grad_(True)\n",
    "optimise(lower_confidence_bound, x_st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
