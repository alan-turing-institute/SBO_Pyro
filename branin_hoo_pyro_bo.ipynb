{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Optimisation with Pyro\n",
    "\n",
    "Goal: apply Bayesian Optimisation (BO) strategy from pyro to minimize Branin-Hoo function as the first step towards structured Bayesian Optimisation implementation in pyro.\n",
    "\n",
    "Based on https://pyro.ai/examples/bo.html\n",
    "\n",
    "The Branin-Hoo function:\n",
    "\n",
    "$f(x) = a(x_2 - bx_1^2 + cx_1 - r)^2 + s(1-t)\\cos(x_1) + s$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "from torch.distributions import constraints, transform_to\n",
    "\n",
    "import pyro\n",
    "import pyro.contrib.gp as gp\n",
    "\n",
    "#assert pyro.__version__.startswith('0.4')\n",
    "pyro.enable_validation(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Branin Hoo constants\n",
    "const_a = 1.0\n",
    "const_b = 5.1 / (4.0 * math.pow(math.pi, 2))\n",
    "const_c = 5.0 / math.pi\n",
    "const_r = 6.0\n",
    "const_s = 10\n",
    "const_t = 1.0 / (8.0 * math.pi)\n",
    "\n",
    "const_x1_min = -5\n",
    "const_x1_max = 10\n",
    "\n",
    "const_x2_min = 0\n",
    "const_x2_max = 15\n",
    "\n",
    "def branin_hoo_first_term(X):\n",
    "    return const_s * (1.0 - const_t) * torch.cos(X[0]) + const_s\n",
    "\n",
    "def branin_hoo(X):\n",
    "    return branin_hoo_first_term(X) + \\\n",
    "        const_a * torch.pow(X[1] - const_b * torch.pow(X[0], 2) + const_c * X[0] - const_r, 2)\n",
    "\n",
    "# Checking LMs\n",
    "assert np.allclose(\n",
    "        branin_hoo(torch.stack((\n",
    "            torch.tensor([-math.pi, math.pi, 9.42478]), \n",
    "            torch.tensor([12.275, 2.275, 2.475])))).numpy(),\n",
    "        np.array([0.397887, 0.397887, 0.397887], dtype=np.float32), rtol=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation of the objective function \n",
    "Impossible in a real-world setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 1000\n",
    "strides = 200\n",
    "\n",
    "X1 = torch.linspace(const_x1_min, const_x1_max, steps)\n",
    "X2 = torch.linspace(const_x2_min, const_x2_max, steps)\n",
    "\n",
    "X1_mesh, X2_mesh = torch.meshgrid(X1, X2)\n",
    "Z_mesh = branin_hoo(torch.stack((X1_mesh, X2_mesh)))z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.contour(X1_mesh, X2_mesh, Z_mesh, strides)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial (train) data\n",
    "\n",
    "Generating random (5) training points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cnt = 10\n",
    "\n",
    "X1_train = torch.tensor([x for x in np.random.uniform(low=const_x1_min, high=const_x1_max, size=train_cnt)])\n",
    "X2_train = torch.tensor([x for x in np.random.uniform(low=const_x2_min, high=const_x2_max, size=train_cnt)])\n",
    "X_train = torch.stack((X1_train, X2_train))\n",
    "\n",
    "Y_train = branin_hoo(X_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gussian Processes as a function for prior\n",
    "\n",
    "Matern kernel with $\\nu=\\frac{5}{2}$ is chosen as the kernel. Other pupolar options are: RBF, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpmodel = gp.models.GPRegression(X_train.T, Y_train, \n",
    "                                 gp.kernels.Matern52(input_dim=2, lengthscale=torch.ones(2)), \n",
    "                                 noise=torch.tensor(0.1), \n",
    "                                 jitter=1.0e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing BO strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining acquisition function\n",
    "\n",
    "Lower Confidence Bound: $\\alpha(x)=\\mu(x) - \\kappa\\sigma(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_confidence_bound(x, kappa=2):\n",
    "    \n",
    "    mu, variance = gpmodel(x, full_cov=False, noiseless=False)\n",
    "    sigma = variance.sqrt()\n",
    "    \n",
    "    return mu - kappa * sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Brooks' implementation of expected improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_phi = lambda x: torch.exp(-x.pow(2)/2)/np.sqrt(2*np.pi)\n",
    "normal_Phi = lambda x: (1 + torch.erf(x / np.sqrt(2))) / 2\n",
    "\n",
    "def expected_improvement(x):\n",
    "    \n",
    "    y_min = gpmodel.y.min()\n",
    "    \n",
    "    mu, variance = gpmodel(x, full_cov=False, noiseless=False)\n",
    "    \n",
    "    sigma = variance.sqrt()\n",
    "    \n",
    "    delta = y_min - mu\n",
    "    \n",
    "    EI = delta.clamp_min(0.0) + sigma*normal_phi(delta/sigma) - delta.abs()*normal_Phi(delta/sigma)\n",
    "    \n",
    "    return -EI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acquisition_func(x):\n",
    "    \n",
    "    #return lower_confidence_bound(x)\n",
    "    \n",
    "    return expected_improvement(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to find minimizing points for an acquisition function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_a_candidate(x_init):\n",
    "    \n",
    "    # Creating constrains\n",
    "    constraint_x1 = constraints.interval(const_x1_min, const_x1_max)\n",
    "    constraint_x2 = constraints.interval(const_x2_min, const_x2_max)\n",
    "    \n",
    "    # transform x_init to an unconstrained domain as we use an unconstrained optimizer\n",
    "    unconstrained_x1_init = transform_to(constraint_x1).inv(x_init[:, 0])\n",
    "    unconstrained_x2_init = transform_to(constraint_x2).inv(x_init[:, 1])\n",
    "    x_uncon_init = torch.stack((unconstrained_x1_init, unconstrained_x2_init), dim=1)\n",
    "    \n",
    "    x_uncon = x_uncon_init.clone().detach().requires_grad_(True)\n",
    "    \n",
    "    # unconstrained minimiser\n",
    "    minimizer = optim.LBFGS([x_uncon])\n",
    "\n",
    "    def closure():\n",
    "        # clear gradients\n",
    "        minimizer.zero_grad()\n",
    "                \n",
    "        x1_tmp = transform_to(constraint_x1)(x_uncon[:, 0])\n",
    "        x2_tmp = transform_to(constraint_x2)(x_uncon[:, 1])\n",
    "        x = torch.stack((x1_tmp, x2_tmp), dim=1)\n",
    "        \n",
    "        y = acquisition_func(x)\n",
    "        \n",
    "        autograd.backward(x_uncon, autograd.grad(y, x_uncon))\n",
    "                \n",
    "        return y\n",
    "    \n",
    "    minimizer.step(closure)\n",
    "        \n",
    "\n",
    "    \n",
    "    # after finding a candidate in the unconstrained domain,\n",
    "    # convert it back to original domain.\n",
    "    x1_tmp = transform_to(constraint_x1)(x_uncon[:, 0])\n",
    "    x2_tmp = transform_to(constraint_x2)(x_uncon[:, 1])\n",
    "    \n",
    "    x = torch.stack((x1_tmp, x2_tmp), dim=1)\n",
    "    \n",
    "    return x.detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A single step of BO\n",
    "\n",
    "LBFGS optimiser used in `find_a_candidate` is a gradient based method and can get stuck at a local minimum. A simple approach to address this is to try several attemps (5) to find the best candidate to minimize the acquisition function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_x(num_candidates=5):\n",
    "    \n",
    "    candidates = []\n",
    "    values = []\n",
    "    \n",
    "    # take the last point as the first attempt\n",
    "    x_init = gpmodel.X[-1:]\n",
    "    \n",
    "    for i in range(num_candidates):\n",
    "        \n",
    "        x = find_a_candidate(x_init)\n",
    "        y = acquisition_func(x)\n",
    "    \n",
    "        candidates.append(x)\n",
    "        values.append(y)\n",
    "        \n",
    "        # a new random attempt initial point\n",
    "        x_init = torch.stack((\n",
    "                x[:,0].new_empty(1).uniform_(const_x1_min, const_x1_max),\n",
    "                x[:,1].new_empty(1).uniform_(const_x2_min, const_x2_max)), dim=1)\n",
    "    \n",
    "        print(\"Candidate \", i, x, y)\n",
    "        \n",
    "    argmin = torch.min(torch.cat(values), dim=0)[1].item()\n",
    "    \n",
    "    print(\"Result: \", candidates[argmin], values[argmin])\n",
    "    \n",
    "    return candidates[argmin]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updating posterior\n",
    "\n",
    "Each time we evaluate `f` at a new value x, we update the `gpmodel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_posterior(x_new):\n",
    "    \n",
    "    # evaluate f at new point\n",
    "    bh_y = branin_hoo(x_new.T) \n",
    "    \n",
    "    # incorporate new evaluation\n",
    "    X = torch.cat([gpmodel.X, x_new]) \n",
    "    y = torch.cat([gpmodel.y, bh_y])\n",
    "    \n",
    "    gpmodel.set_data(X, y)\n",
    "    \n",
    "    # optimize the GP hyperparameters using Adam with lr=0.001\n",
    "    optimizer = torch.optim.Adam(gpmodel.parameters(), lr=0.001)\n",
    "    \n",
    "    gp.util.train(gpmodel, optimizer) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The BO Algorithm (minimalistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bo_steps = 10\n",
    "\n",
    "optimizer = torch.optim.Adam(gpmodel.parameters(), lr=0.001)\n",
    "gp.util.train(gpmodel, optimizer)\n",
    "\n",
    "for i in range(bo_steps):\n",
    "    print(\"-\"*50)\n",
    "    print(\"BO STEP: \", i)\n",
    "    print(\"-\"*50)\n",
    "    xmin = next_x()\n",
    "    update_posterior(xmin)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing objective function vs GP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 6), ncols=2, nrows=1)\n",
    "ax[0].contour(X1_mesh, X2_mesh, Z_mesh, strides)\n",
    "ax[0].title.set_text('Branin-Hoo')\n",
    "\n",
    "predict_mesh, _ = gpmodel(torch.stack((X1_mesh.flatten(), X2_mesh.flatten())).T)\n",
    "ax[1].contour(X1_mesh, X2_mesh, predict_mesh.reshape((steps, steps)).detach(), strides)\n",
    "ax[1].title.set_text('GP (mean)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualising BO x points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_points(bo_ax, gpmodel_, train_cnt):\n",
    "    \n",
    "    # Plotting training data\n",
    "    bo_ax.scatter(\n",
    "        gpmodel_.X.numpy().T[0][:train_cnt], \n",
    "        gpmodel_.X.numpy().T[1][:train_cnt],\n",
    "        marker=\"x\", s=500, c='black')\n",
    "\n",
    "    # Plotting BO steps\n",
    "    bo_ax.scatter(\n",
    "        gpmodel_.X.numpy().T[0][train_cnt:], \n",
    "        gpmodel_.X.numpy().T[1][train_cnt:],\n",
    "        marker=\"o\", s=100, c='red')\n",
    "\n",
    "    for i, iter in enumerate(gpmodel_.X.numpy()):\n",
    "        if i >= train_cnt:\n",
    "            bo_ax.annotate(\"%d\" % (i+1-train_cnt), \n",
    "                        (iter.T[0] + 0.1, iter.T[1] + 0.1))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 9))\n",
    "CS = ax.contour(X1_mesh, X2_mesh, Z_mesh, strides)\n",
    "\n",
    "add_points(ax, gpmodel, train_cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualising BO Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bo_steps = 10\n",
    "# no_cols = 3\n",
    "\n",
    "# fbo, fbo_axes = plt.subplots(ncols=no_cols, nrows=bo_steps, figsize=(no_cols*6, bo_steps*6))\n",
    "\n",
    "# optimizer = torch.optim.Adam(gpmodel.parameters(), lr=0.001)\n",
    "# gp.util.train(gpmodel, optimizer)\n",
    "\n",
    "# for i in range(bo_steps):\n",
    "\n",
    "#     xmin = next_x()\n",
    "    \n",
    "#     ########### Ploting prior\n",
    "#     Predict_mesh, _ = gpmodel(torch.stack((X1_mesh.flatten(), X2_mesh.flatten())).T) \n",
    "\n",
    "#     Predict_mesh = Predict_mesh.reshape((steps, steps)).detach()\n",
    "#     fbo_axes[i, 0].contour(X1_mesh, X2_mesh, Predict_mesh, strides)\n",
    "#     fbo_axes[i, 0].title.set_text('Step %d: prior' % (i+1))\n",
    "    \n",
    "#     ########### Plotting acquisition function\n",
    "#     acquisition_mesh = acquisition_func(\n",
    "#             torch.stack((X1_mesh.flatten(), X2_mesh.flatten())).T)\n",
    "    \n",
    "#     acquisition_mesh = acquisition_mesh.reshape((steps, steps)).detach()\n",
    "#     fbo_axes[i, 1].contour(X1_mesh, X2_mesh, acquisition_mesh, strides)\n",
    "    \n",
    "#     # Updating posterior\n",
    "#     update_posterior(xmin)\n",
    "            \n",
    "#     ########### Plotting GP countour plot\n",
    "#     Predict_mesh, _ = gpmodel(torch.stack((X1_mesh.flatten(), X2_mesh.flatten())).T) \n",
    "\n",
    "#     Predict_mesh = Predict_mesh.reshape((steps, steps)).detach()\n",
    "#     fbo_axes[i, 2].contour(X1_mesh, X2_mesh, Predict_mesh, strides)\n",
    "#     fbo_axes[i, 2].title.set_text('Step %d: posterior' % (i+1))\n",
    "    \n",
    "#     fbo_axes[i, 1].scatter(\n",
    "#         gpmodel.X.numpy().T[0][len(gpmodel.X.numpy())-1], \n",
    "#         gpmodel.X.numpy().T[1][len(gpmodel.X.numpy())-1],\n",
    "#         marker=\"o\", s=100, c='red')\n",
    "    \n",
    "#     fbo_axes[i, 1].title.set_text('Step %d: acquisition function' % (i+1))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
