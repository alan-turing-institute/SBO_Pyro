{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Optimisation with Pyro 1.0  using GPs on the Branin-Hoo example\n",
    "\n",
    "**Goal**: apply Bayesian Optimisation (BO) strategy to minimize Branin-Hoo function with pyro.\n",
    "\n",
    "The Branin-Hoo function:\n",
    "\n",
    "$f(x) = a(x_2 - bx_1^2 + cx_1 - r)^2 + s(1-t)\\cos(x_1) + s$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "from torch.distributions import constraints, transform_to\n",
    "\n",
    "import pyro\n",
    "import pyro.contrib.gp as gp\n",
    "import pyro.distributions as dist\n",
    "\n",
    "pyro.enable_validation(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"torch:\", torch.__version__)\n",
    "print(\"pyro:\", pyro.__version__)\n",
    "\n",
    "if not pyro.__version__.startswith(\"1\"):\n",
    "    raise ValueError(\"incompatible version of pyro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_number = 444\n",
    "\n",
    "def set_random_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "set_random_seed(seed_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const_x1_min = -5\n",
    "const_x1_max = 10\n",
    "\n",
    "const_x2_min = 0\n",
    "const_x2_max = 15\n",
    "\n",
    "def branin_hoo(x):\n",
    "    \"\"\" Compute Branin-Hoo function for fixed constants \"\"\"\n",
    "    \n",
    "    a = 1.0\n",
    "    b = 5.1 / (4 * np.pi**2)\n",
    "    c = 5.0 / np.pi\n",
    "    r = 6.0\n",
    "    s = 10.0\n",
    "    t = 1.0 / (8 * np.pi)\n",
    "    \n",
    "    x1 = x[..., 0]\n",
    "    x2 = x[..., 1]\n",
    "    \n",
    "    return a * (x2 - b*x1**2 + c*x1 - r)**2 + s*(1 - t)*torch.cos(x1) + s\n",
    "\n",
    "# Checking LMs\n",
    "assert np.allclose(\n",
    "        branin_hoo(torch.stack((\n",
    "            torch.tensor([-math.pi, math.pi, 9.42478]), \n",
    "            torch.tensor([12.275, 2.275, 2.475])), dim=1)).numpy(),\n",
    "        np.array([0.397887, 0.397887, 0.397887], dtype=np.float32), rtol=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation of the objective function and training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cnt = 10\n",
    "\n",
    "X1_train = torch.tensor([x for x in np.random.uniform(low=const_x1_min, high=const_x1_max, size=train_cnt)])\n",
    "X2_train = torch.tensor([x for x in np.random.uniform(low=const_x2_min, high=const_x2_max, size=train_cnt)])\n",
    "X_train = torch.stack((X1_train, X2_train), dim=1)\n",
    "Y_train = branin_hoo(X_train)\n",
    "\n",
    "def plot_target():\n",
    "\n",
    "    steps = 1000\n",
    "    strides = 200\n",
    "\n",
    "    X1 = torch.linspace(const_x1_min, const_x1_max, steps)\n",
    "    X2 = torch.linspace(const_x2_min, const_x2_max, steps)\n",
    "\n",
    "    X1_mesh, X2_mesh = torch.meshgrid(X1, X2)\n",
    "    Z_mesh = branin_hoo(torch.stack((X1_mesh, X2_mesh), dim=2))\n",
    "\n",
    "    plt.contour(X1_mesh, X2_mesh, Z_mesh, strides)\n",
    "    plt.colorbar()\n",
    "    \n",
    "plot_target()\n",
    "    \n",
    "# Plotting training data\n",
    "plt.scatter(X1_train, X2_train, marker=\"x\", s=200, c='orange', zorder=2, linewidth=4);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing BO strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining acquisition function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_confidence_bound(gpmodel, x, kappa=2):\n",
    "    \"\"\"\n",
    "    Lower Confidence Bound (LCB): $\\alpha(x)=\\mu(x) - \\kappa\\sigma(x)$\n",
    "    \n",
    "    \"\"\"\n",
    "    mu, variance = gpmodel(x, full_cov=False, noiseless=False)\n",
    "    sigma = variance.sqrt()\n",
    "    \n",
    "    return mu - kappa * sigma\n",
    "\n",
    "normal_phi = lambda x: torch.exp(-x.pow(2)/2)/np.sqrt(2*np.pi)\n",
    "normal_Phi = lambda x: (1 + torch.erf(x / np.sqrt(2))) / 2\n",
    "\n",
    "def expected_improvement(gpmodel, x):\n",
    "    \"\"\"\n",
    "    Brooks' implementation of expected improvement (EI).\n",
    "    \n",
    "    \"\"\"\n",
    "    y_min = gpmodel.y.min()\n",
    "    \n",
    "    mu, variance = gpmodel(x, full_cov=False, noiseless=False)\n",
    "    \n",
    "    sigma = variance.sqrt()\n",
    "    \n",
    "    delta = y_min - mu\n",
    "    \n",
    "    EI = delta.clamp_min(0.0) + sigma*normal_phi(delta/sigma) - delta.abs()*normal_Phi(delta/sigma)\n",
    "    \n",
    "    return -EI\n",
    "\n",
    "def acquisition_func(gpmodel, x, af='EI'):\n",
    "    \"\"\"\n",
    "    Defines acquisition function.\n",
    "    \"\"\"\n",
    "    \n",
    "    if af == \"EI\":\n",
    "        return expected_improvement(gpmodel, x)\n",
    "    \n",
    "    elif af == \"LCB\":\n",
    "        return lower_confidence_bound(gpmodel, x)\n",
    "    \n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimalistic BO Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to find minimizing points for an acquisition function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_a_candidate(gpmodel, x_init):\n",
    "    \n",
    "    # Creating constrains\n",
    "    constraint_x1 = constraints.interval(const_x1_min, const_x1_max)\n",
    "    constraint_x2 = constraints.interval(const_x2_min, const_x2_max)\n",
    "    \n",
    "    # transform x_init to an unconstrained domain as we use an unconstrained optimizer\n",
    "    unconstrained_x1_init = transform_to(constraint_x1).inv(x_init[:, 0])\n",
    "    unconstrained_x2_init = transform_to(constraint_x2).inv(x_init[:, 1])\n",
    "    x_uncon_init = torch.stack((unconstrained_x1_init, unconstrained_x2_init), dim=1)\n",
    "    \n",
    "    x_uncon = x_uncon_init.clone().detach().requires_grad_(True)\n",
    "    \n",
    "    # unconstrained minimiser\n",
    "    minimizer = optim.Adam([x_uncon])\n",
    "\n",
    "    def closure():\n",
    "        # clear gradients\n",
    "        minimizer.zero_grad()\n",
    "                \n",
    "        x1_tmp = transform_to(constraint_x1)(x_uncon[:, 0])\n",
    "        x2_tmp = transform_to(constraint_x2)(x_uncon[:, 1])\n",
    "        x = torch.stack((x1_tmp, x2_tmp), dim=1)\n",
    "        \n",
    "        y = acquisition_func(gpmodel, x)\n",
    "        \n",
    "        autograd.backward(x_uncon, autograd.grad(y, x_uncon))\n",
    "                \n",
    "        return y\n",
    "    \n",
    "    for _ in range(100):\n",
    "        minimizer.step(closure)\n",
    "        \n",
    "    # after finding a candidate in the unconstrained domain,\n",
    "    # convert it back to original domain.\n",
    "    x1_tmp = transform_to(constraint_x1)(x_uncon[:, 0])\n",
    "    x2_tmp = transform_to(constraint_x2)(x_uncon[:, 1])\n",
    "    \n",
    "    x = torch.stack((x1_tmp, x2_tmp), dim=1)\n",
    "    \n",
    "    return x.detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A single step of BO\n",
    "\n",
    "LBFGS optimiser used in `find_a_candidate` is a gradient based method and can get stuck at a local minimum. A simple approach to address this is to try several attemps (5) to find the best candidate to minimize the acquisition function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_x(gpmodel, num_candidates=5):\n",
    "    \n",
    "    candidates = []\n",
    "    values = []\n",
    "    \n",
    "    # take the best (lowest) point as the first attempt\n",
    "    x_init = gpmodel.X[[gpmodel.y.argmin()], :].detach().requires_grad_(True)\n",
    "        \n",
    "    for i in range(num_candidates):\n",
    "        \n",
    "        x = find_a_candidate(gpmodel, x_init)\n",
    "        y = acquisition_func(gpmodel, x)\n",
    "    \n",
    "        candidates.append(x)\n",
    "        values.append(y)\n",
    "        \n",
    "        # a new random attempt initial point\n",
    "        x_init = torch.stack((\n",
    "                x[:,0].new_empty(1).uniform_(const_x1_min, const_x1_max),\n",
    "                x[:,1].new_empty(1).uniform_(const_x2_min, const_x2_max)), dim=1)\n",
    "    \n",
    "        print(\"Candidate \", i, x, y)\n",
    "        \n",
    "    argmin = torch.min(torch.cat(values), dim=0)[1].item()\n",
    "    \n",
    "    print(\"Result: \", candidates[argmin], values[argmin])\n",
    "    \n",
    "    return candidates[argmin], candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating posterior\n",
    "\n",
    "Each time we evaluate `f` at a new value x, we update the `gpmodel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_posterior(gpmodel, x_new, svi_mode=False):\n",
    "        \n",
    "    # evaluate f at new point\n",
    "    bh_y = branin_hoo(x_new) \n",
    "    \n",
    "    # incorporate new evaluation\n",
    "    X = torch.cat([gpmodel.X, x_new]) \n",
    "    y = torch.cat([gpmodel.y, bh_y])\n",
    "        \n",
    "    gpmodel.set_data(X, y)\n",
    "    \n",
    "    # optimising hyper paramters\n",
    "    \n",
    "    optimiser = torch.optim.Adam(gpmodel.parameters(), lr=0.001)\n",
    "    \n",
    "    if svi_mode:\n",
    "        loss_fn = pyro.infer.TraceMeanField_ELBO().differentiable_loss\n",
    "        gp.util.train(gpmodel, optimiser, loss_fn, num_steps=2000)\n",
    "    else:\n",
    "        gp.util.train(gpmodel, optimiser) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_function(f, x1_min, x1_max, x2_min, x2_max, n_points=100, strides=200):\n",
    "    \n",
    "    X1 = torch.linspace(x1_min, x1_max, n_points)\n",
    "    X2 = torch.linspace(x2_min, x2_max, n_points)\n",
    "\n",
    "    X1_mesh, X2_mesh = torch.meshgrid(X1, X2)\n",
    "    \n",
    "    Z_mesh = f(torch.stack((X1_mesh.flatten(), X2_mesh.flatten()), dim=1)).reshape(n_points,n_points)\n",
    "    \n",
    "    plt.contourf(X1_mesh, X2_mesh, Z_mesh, strides)\n",
    "    plt.set_cmap('jet')\n",
    "    plt.colorbar()\n",
    "    \n",
    "def visualise_step(gp_model_before, xcans, gp_model_after):\n",
    "    \n",
    "    plt.figure(figsize=(15, 3)) \n",
    "    \n",
    "    plt.subplot(1,5,1)\n",
    "    plt.title(\"Acquisition \\n(Before)\")\n",
    "    \n",
    "    plt.scatter(gp_model_before.X.numpy().T[0], \n",
    "                gp_model_before.X.numpy().T[1], \n",
    "                marker=\"x\", s=200, c='orange', zorder=2, linewidth=4);\n",
    "    \n",
    "    #  marking candidates\n",
    "    for can in xcans:\n",
    "        plt.scatter(can.numpy().T[0], \n",
    "            can.numpy().T[1], \n",
    "            marker=\"x\", s=200, c='white', zorder=3, linewidth=4);\n",
    "    \n",
    "    with torch.no_grad(): \n",
    "        f = lambda x: acquisition_func(gp_model_before, x)\n",
    "        plot_function(f, const_x1_min, const_x1_max, const_x2_min, const_x2_max)\n",
    "    \n",
    "    plt.subplot(1,5,2)\n",
    "    plt.title(\"New point on the \\ntarget function's landscape\")\n",
    "    plot_target()\n",
    "    plt.scatter(gp_model_after.X.numpy().T[0][-1], \n",
    "            gp_model_after.X.numpy().T[1][-1], \n",
    "            marker=\"x\", s=200, c='black', zorder=3, linewidth=4);\n",
    "    \n",
    "    plt.subplot(1,5,3)\n",
    "    plt.title(\"Acquisition \\n(After)\")\n",
    "    \n",
    "    plt.scatter(gp_model_after.X.numpy().T[0][-1], \n",
    "                gp_model_after.X.numpy().T[1][-1], \n",
    "                marker=\"x\", s=200, c='black', zorder=3, linewidth=4);\n",
    "    \n",
    "    with torch.no_grad(): \n",
    "        f = lambda x: acquisition_func(gp_model_after, x)\n",
    "        plot_function(f, const_x1_min, const_x1_max, const_x2_min, const_x2_max)\n",
    "            \n",
    "    with torch.no_grad(), gp_model_after._pyro_context:\n",
    "        plt.subplot(1,5,4)\n",
    "        plt.title(\"GP mean\")\n",
    "        f = lambda x: gp_model_after(x)[0]\n",
    "        plot_function(f, const_x1_min, const_x1_max, const_x2_min, const_x2_max)\n",
    "\n",
    "        plt.subplot(1,5,5)\n",
    "        plt.title(\"GP variance\")\n",
    "        f = lambda x: gp_model_after(x)[1]\n",
    "        plot_function(f, const_x1_min, const_x1_max, const_x2_min, const_x2_max)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deterministic approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_random_seed(333)\n",
    "\n",
    "gp_model = gp.models.GPRegression(X_train, Y_train, \n",
    "                                 gp.kernels.Matern52(input_dim=2, lengthscale=torch.ones(2)), \n",
    "                                 noise=torch.tensor(0.1), \n",
    "                                 jitter=1.0e-4)\n",
    "\n",
    "optimizer = torch.optim.Adam(gp_model.parameters(), lr=0.001)\n",
    "gp.util.train(gp_model, optimizer);\n",
    "\n",
    "bo_steps = 10\n",
    "\n",
    "for i in range(bo_steps):\n",
    "    print(\"-\"*50)\n",
    "    print(\"BO STEP: \", i)\n",
    "    print(\"-\"*50)\n",
    "    \n",
    "    gp_model_ = copy.copy(gp_model)\n",
    "    \n",
    "    xmin, xcans = next_x(gp_model)\n",
    "    \n",
    "    update_posterior(gp_model, xmin)\n",
    "        \n",
    "    # visualising the step\n",
    "    visualise_step(gp_model_, xcans, gp_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_random_seed(333)\n",
    "\n",
    "pyro.clear_param_store()\n",
    "gp_model_svi = gp.models.GPRegression(X_train, Y_train, gp.kernels.Matern52(input_dim=2, lengthscale=torch.ones(2)))\n",
    "\n",
    "# Set priors\n",
    "gp_model_svi.kernel.lengthscale = pyro.nn.PyroSample(dist.LogNormal(0, 1).expand([2]).to_event())\n",
    "gp_model_svi.kernel.variance = pyro.nn.PyroSample(dist.LogNormal(0, 1))\n",
    "gp_model_svi.noise = pyro.nn.PyroSample(dist.LogNormal(0, 1))\n",
    "\n",
    "# Set guides\n",
    "gp_model_svi.kernel.autoguide(\"lengthscale\", dist.Normal)\n",
    "gp_model_svi.kernel.autoguide(\"variance\", dist.Normal)\n",
    "gp_model_svi.autoguide(\"noise\", dist.Normal)\n",
    "\n",
    "# optimise\n",
    "optimizer = torch.optim.Adam(gp_model_svi.parameters(), lr=0.005)\n",
    "loss_fn = pyro.infer.TraceMeanField_ELBO().differentiable_loss\n",
    "losses = gp.util.train(gp_model_svi, optimizer, loss_fn, num_steps=2000)\n",
    "\n",
    "bo_steps = 10\n",
    "\n",
    "for i in range(bo_steps):\n",
    "    print(\"-\"*50)\n",
    "    print(\"BO STEP: \", i)\n",
    "    print(\"-\"*50)\n",
    "    \n",
    "    gp_model_svi_ = copy.copy(gp_model_svi)\n",
    "    \n",
    "    xmin, xcans = next_x(gp_model_svi)\n",
    "    \n",
    "    update_posterior(gp_model_svi, xmin, svi_mode=True)\n",
    "        \n",
    "    # visualising the step\n",
    "    visualise_step(gp_model_svi_, xcans, gp_model_svi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
