{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "import torch\n",
    "import pyro\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "from torch.distributions import constraints, transform_to\n",
    "\n",
    "import pyro.distributions as dist\n",
    "import pyro.contrib.gp as gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_number = 333\n",
    "\n",
    "def set_random_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "set_random_seed(seed_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = -5\n",
    "b = 0\n",
    "c = 0.5\n",
    "\n",
    "def test_function(X):\n",
    "    return a * torch.exp(-1.0 * torch.pow((X - b), 2) / (2*c*c))\n",
    "\n",
    "const_x1_min = -5.0\n",
    "const_x1_max = 5.0\n",
    "\n",
    "x_ = torch.linspace(const_x1_min, const_x1_max, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cnt = 3\n",
    "X_train = torch.tensor([x for x in np.random.uniform(low=const_x1_min, high=const_x1_max, size=train_cnt)])\n",
    "Y_train = test_function(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualising objective function and training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_, test_function(x_))\n",
    "plt.plot(X_train, Y_train, \"*\", markersize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gp_model(X, Y, plot_opti=False):\n",
    "    D = 1\n",
    "    gp_model = gp.models.GPRegression(X, Y, \n",
    "                                      kernel=gp.kernels.Matern52(input_dim=D, lengthscale=torch.ones(D)))\n",
    "    ###############\n",
    "    # Set priors\n",
    "    ###############\n",
    "    gp_model.kernel.set_prior(\"lengthscale\", dist.LogNormal(0.0, 1.0).expand((D,)).to_event(1))\n",
    "    gp_model.kernel.set_prior(\"variance\", dist.LogNormal(0.0, 1.0))\n",
    "    #    Assuming noiseless model\n",
    "    gp_model.set_prior(\"noise\", dist.Uniform(0.0, 1.0))\n",
    "\n",
    "    ###############\n",
    "    # Set guides\n",
    "    ###############\n",
    "    gp_model.kernel.autoguide(\"lengthscale\", dist.Normal)\n",
    "    gp_model.kernel.autoguide(\"variance\", dist.Normal)\n",
    "    gp_model.autoguide(\"noise\", dist.Normal)\n",
    "    \n",
    "    # Optimiser\n",
    "    gp_opt = torch.optim.Adam(gp_model.parameters(), lr=0.005)\n",
    "    loss_fn = pyro.infer.Trace_ELBO().differentiable_loss\n",
    "    \n",
    "    losses = []\n",
    "\n",
    "    num_steps = 5000\n",
    "    for i in range(num_steps):\n",
    "        gp_opt.zero_grad()\n",
    "\n",
    "        loss = loss_fn(gp_model.model, gp_model.guide)\n",
    "        loss.backward()\n",
    "\n",
    "        gp_opt.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "    \n",
    "    if plot_opti:\n",
    "        plt.semilogy(losses);\n",
    "    \n",
    "    return gp_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_model = create_gp_model(X_train, Y_train, plot_opti=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(list(gp_model.named_buffers()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_result = gp_model(x_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(x_, predict_result[0].detach().numpy(), yerr=predict_result[1].detach().numpy(), color=\"#1f77b4\")\n",
    "plt.plot(x_, test_function(x_), color=\"orange\")\n",
    "plt.plot(X_train, Y_train, \"*\", markersize=20, color=\"green\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expected Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_phi = lambda x: torch.exp(-x.pow(2)/2)/np.sqrt(2*np.pi)\n",
    "normal_Phi = lambda x: (1 + torch.erf(x / np.sqrt(2))) / 2\n",
    "  \n",
    "def expected_improvement(gpmodel, x):\n",
    "    \n",
    "    y_min = gpmodel.y.min()\n",
    "    \n",
    "    mu, variance = gpmodel(x, full_cov=False, noiseless=False)\n",
    "    \n",
    "    sigma = variance.sqrt()\n",
    "    \n",
    "    delta = y_min - mu\n",
    "    \n",
    "    EI = delta.clamp_min(0.0) + sigma*normal_phi(delta/sigma) - delta.abs()*normal_Phi(delta/sigma)\n",
    "    \n",
    "    return -EI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ei_result = expected_improvement(gp_model, x_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,6))\n",
    "plt.errorbar(x_, \n",
    "             predict_result[0].detach().numpy(), \n",
    "             yerr=predict_result[1].detach().numpy(), \n",
    "             color=\"#1f77b4\", label=\"GP model\")\n",
    "\n",
    "plt.plot(x_, test_function(x_), color=\"orange\", label=\"Target\")\n",
    "plt.plot(X_train, Y_train, \"*\", markersize=20, color=\"green\", label=\"Train\")\n",
    "plt.plot(x_, ei_result.detach().numpy(), color=\"red\", label=\"EI\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AVG Expected Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_expected_improvement(gpmodel, x, no_samples=10):\n",
    "\n",
    "    D = x.shape[0]\n",
    "    EI_sum = torch.zeros((D))\n",
    "    \n",
    "    for _ in range(no_samples):\n",
    "        EI_sum += expected_improvement(gpmodel, x)\n",
    "    \n",
    "    EI_sum /= no_samples\n",
    "    \n",
    "    return EI_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_ei_result = avg_expected_improvement(gp_model, x_, no_samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,6))\n",
    "plt.errorbar(x_, \n",
    "             predict_result[0].detach().numpy(), \n",
    "             yerr=predict_result[1].detach().numpy(), \n",
    "             color=\"#1f77b4\", label=\"GP model\")\n",
    "\n",
    "plt.plot(x_, test_function(x_), color=\"orange\", label=\"Target\")\n",
    "plt.plot(X_train, Y_train, \"*\", markersize=20, color=\"green\", label=\"Train\")\n",
    "\n",
    "plt.plot(x_, avg_ei_result.detach().numpy(), color=\"red\", label=\"AVG EI\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing EI and avg EI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,6))\n",
    "plt.plot(x_, ei_result.detach().numpy(), color=\"blue\", label=\"EI\")\n",
    "plt.plot(x_, avg_ei_result.detach().numpy(), color=\"red\", label=\"AVG EI\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing optimisation between EI and AVG EI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimise(acquisition_func, x_st, lr=1.0):\n",
    "    \n",
    "    # unconstrained minimiser\n",
    "    minimizer = optim.LBFGS([x_st], lr=lr)\n",
    "                        \n",
    "    def closure():\n",
    "        # clear gradients\n",
    "        minimizer.zero_grad()\n",
    "\n",
    "        y = acquisition_func(x_st)\n",
    "\n",
    "        autograd.backward(x_st, autograd.grad(y, x_st))\n",
    "\n",
    "        print(\"x_st\", x_st, y)\n",
    "\n",
    "        return y\n",
    "\n",
    "    minimizer.step(closure)\n",
    "    \n",
    "    return minimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_st = torch.Tensor([-1.5]).detach().requires_grad_(True)\n",
    "optimise(lambda x: expected_improvement(gp_model, x), x_st, lr=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_st = torch.Tensor([-1.5]).detach().requires_grad_(True)\n",
    "opti_resul = optimise(lambda x: avg_expected_improvement(gp_model, x), x_st, lr=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing BO with EI and avg EI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimalistic BO approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_a_candidate(acquisition_func, x_init, x_min, x_max):\n",
    "    # Creating constrains\n",
    "    constraint_x = constraints.interval(x_min, x_max)\n",
    "    \n",
    "    # transform x_init to an unconstrained domain as we use an unconstrained optimizer\n",
    "    x_uncon_init = transform_to(constraint_x).inv(x_init)\n",
    "    x_uncon = x_uncon_init.clone().detach().requires_grad_(True)\n",
    "    \n",
    "    # unconstrained minimiser\n",
    "    minimizer = optim.LBFGS([x_uncon])\n",
    "\n",
    "    def closure():\n",
    "        # clear gradients\n",
    "        minimizer.zero_grad()\n",
    "        x = transform_to(constraint_x)(x_uncon)\n",
    "        y = acquisition_func(x)\n",
    "        autograd.backward(x_uncon, autograd.grad(y, x_uncon))\n",
    "                        \n",
    "        return y\n",
    "    \n",
    "    minimizer.step(closure)\n",
    "        \n",
    "    # after finding a candidate in the unconstrained domain,\n",
    "    # convert it back to original domain.\n",
    "    x = transform_to(constraint_x)(x_uncon)\n",
    "        \n",
    "    return x.detach()\n",
    "\n",
    "def next_x(acquisition_func, gp_model, x_init, x_min, x_max, num_candidates=5):\n",
    "    \n",
    "    candidates = []\n",
    "    values = []\n",
    "    \n",
    "    for _ in range(num_candidates):\n",
    "        \n",
    "        x_can = find_a_candidate(lambda x: acquisition_func(gp_model, x), \n",
    "                                 x_init, x_min, x_max)\n",
    "        \n",
    "        y_can = test_function(x_can)\n",
    "        \n",
    "        candidates.append(x_can)\n",
    "        values.append(y_can)\n",
    "        \n",
    "        x_init = torch.tensor([x for x in np.random.uniform(low=x_min, high=x_max, size=1)])\n",
    "    \n",
    "    #print(\"candidates: \", candidates)\n",
    "    #print(\"values: \", values)\n",
    "    \n",
    "    argmin = torch.min(torch.cat(values), dim=0)[1].item()\n",
    "    candidate = candidates[argmin]\n",
    "    \n",
    "    return candidate\n",
    "\n",
    "def update_posterior(gpmodel, x_new, y_new):\n",
    "    \n",
    "    # incorporate new evaluation\n",
    "    X = torch.cat([gpmodel.X, x_new]) \n",
    "    y = torch.cat([gpmodel.y, y_new])\n",
    "    \n",
    "    gpmodel.set_data(X, y)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(gpmodel.parameters(), lr=0.005)\n",
    "    \n",
    "    gp.util.train(gpmodel, optimizer)\n",
    "    \n",
    "    return X, y,\n",
    "\n",
    "def BO(gp_model, acquisition_function, X, Y, bo_steps=5, num_candidates=5):\n",
    "    \n",
    "    X_train_ = copy.copy(X)\n",
    "    Y_train_ = copy.copy(Y)\n",
    "\n",
    "    for i in range(bo_steps):\n",
    "\n",
    "        x_init = torch.Tensor([X_train_[-1]]).detach().requires_grad_(True)\n",
    "\n",
    "        x_new = next_x(acquisition_function, gp_model, x_init, \n",
    "                       const_x1_min, const_x1_max, num_candidates=num_candidates)\n",
    "\n",
    "        y_new = test_function(x_new) \n",
    "\n",
    "        print(\"BO STEP: \", i+1, \n",
    "              \"ini = \", x_init.detach().numpy()[0], \n",
    "              \"fin = \", x_new.detach().numpy()[0], \n",
    "              \"value = \", y_new.detach().numpy()[0]\n",
    "        )\n",
    "\n",
    "        X_train_ = torch.cat([X_train_, x_new]) \n",
    "        Y_train_ = torch.cat([Y_train_, y_new]) \n",
    "\n",
    "        update_posterior(gp_model, x_new, y_new)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BO AVG EI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_random_seed(444)\n",
    "gp_model = create_gp_model(X_train, Y_train)\n",
    "\n",
    "BO(gp_model, avg_expected_improvement, X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BO EI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_random_seed(444)\n",
    "gp_model = create_gp_model(X_train, Y_train)\n",
    "\n",
    "BO(gp_model, expected_improvement, X_train, Y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
