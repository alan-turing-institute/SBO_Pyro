{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semi-parametric model in Pyro\n",
    "\n",
    "\n",
    "Goal: implement the Branin-Hoo example from https://github.com/VDalibard/BOAT in pyro, with both nonparametric and semi-parametric models. \n",
    "\n",
    "Also borrows from the example at https://pyro.ai/examples/bo.html. First step is to implement a \"structued\" GP.\n",
    "\n",
    "The Branin-Hoo function:\n",
    "\n",
    "$f(x) = a(x_2 - bx_1^2 + cx_1 - r)^2 + s(1-t)\\cos(x_1) + s$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import constraints, transform_to\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import pyro.contrib.gp as gp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(torch.__version__)\n",
    "print(pyro.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def branin_hoo(x):\n",
    "    \"\"\" Compute Branin-Hoo function for fixed constants \"\"\"\n",
    "    a = 1.0\n",
    "    b = 5.1 / (4 * np.pi**2)\n",
    "    c = 5.0 / np.pi\n",
    "    r = 6.0\n",
    "    s = 10.0\n",
    "    t = 1.0 / (8 * np.pi)\n",
    "    x1 = x[...,0]\n",
    "    x2 = x[...,1]\n",
    "    return a * (x2 - b*x1**2 + c*x1 - r)**2 + s*(1 - t)*torch.cos(x1) + s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the test function everywhere (obviously can't do this in real-world setting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_function(f, n_points=100):\n",
    "    XX, YY = np.meshgrid(np.linspace(-5, 10, n_points), np.linspace(0, 15, n_points))\n",
    "    ZZ = f(torch.FloatTensor(np.stack([XX.ravel(), YY.ravel()]).T))\n",
    "    plt.imshow(ZZ.reshape(n_points, n_points));\n",
    "    plt.xticks(np.linspace(0, n_points, 6), np.linspace(-5, 10, 6))\n",
    "    plt.yticks(np.linspace(0, n_points, 6), np.linspace(0, 15, 6))\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    plt.set_cmap('jet')\n",
    "    plt.colorbar()\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plot_function(branin_hoo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before trying Bayesian optimization, let's try different GP models in Pyro.\n",
    "\n",
    "Would be good to understand how to use the following in Pyro:\n",
    "\n",
    "1. Standard (nonparametric) GP model, optimizing hyperparameters\n",
    "2. Standard GP model, but running approximate inference over the hyperparameters (i.e. estimating a posterior distribution)\n",
    "3. Semi-parametric GP model, following the example BOAT code, running inference over all parameters.\n",
    "\n",
    "Hopefully we can see how easy it is to fit these models in Pyro, and the differences between the learned models for some fixed set of data.\n",
    "\n",
    "## Standard GP model, optimizing hyperparameters\n",
    "\n",
    "Initialize with some random points, then try out a GP.\n",
    "\n",
    "Use a Matern kernel (an arbitrary choice, at this point) and lognormal priors on all parameters (also an arbitrary choice)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.random.manual_seed(666)\n",
    "\n",
    "N_points = 10\n",
    "X = torch.rand(N_points, 2)*15 + torch.FloatTensor([-5, 0])\n",
    "y = branin_hoo(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpmodel = gp.models.GPRegression(X, y, gp.kernels.Matern52(input_dim=2, lengthscale=torch.ones(2)))\n",
    "gpmodel.kernel.set_prior(\"lengthscale\", dist.LogNormal(0.0, 1.0))\n",
    "gpmodel.kernel.set_prior(\"variance\", dist.LogNormal(0.0, 1.0))\n",
    "gpmodel.set_prior(\"noise\", dist.LogNormal(0.0, 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is what the GP looks like before optimizing the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,3.5))\n",
    "with torch.no_grad():\n",
    "    plt.subplot(121)\n",
    "    plt.title(\"GP mean\")\n",
    "    plot_function(lambda X: gpmodel(X)[0])\n",
    "    plt.subplot(122)\n",
    "    plt.title(\"GP variance\")\n",
    "    plot_function(lambda X: gpmodel(X)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now find MAP estimates of the GP parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAP - maximum a posteriori probability  estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(gpmodel.parameters(), lr=0.005)\n",
    "loss_fn = pyro.infer.Trace_ELBO().differentiable_loss\n",
    "losses = []\n",
    "num_steps = 5000\n",
    "for i in range(num_steps):\n",
    "    optimizer.zero_grad()\n",
    "    loss = loss_fn(gpmodel.model, gpmodel.guide)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss.item())\n",
    "plt.semilogy(losses);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('variance = {}'.format(gpmodel.kernel.variance))\n",
    "print('lengthscale = {}'.format(gpmodel.kernel.lengthscale))\n",
    "print('noise = {}'.format(gpmodel.noise))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,3.5))\n",
    "with torch.no_grad():\n",
    "    plt.subplot(121)\n",
    "    plt.title(\"GP mean\")\n",
    "    plot_function(lambda X: gpmodel(X)[0])\n",
    "    plt.subplot(122)\n",
    "    plt.title(\"GP variance\")\n",
    "    plot_function(lambda X: gpmodel(X)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard GP model, doing inference over hyperparameters\n",
    "\n",
    "This is more along the lines of what is being done in the BOAT baseline nonparametric model. \n",
    "\n",
    "Instead of optimizing the GP parameters, we will try to approximate the posteriors of each with a normal distribution.\n",
    "\n",
    "This is actually very simple to do â€” we just have to define an `autoguide` for each parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpmodel_svi = gp.models.GPRegression(X, y, gp.kernels.Matern52(input_dim=2, lengthscale=torch.ones(2)))\n",
    "\n",
    "# Set priors\n",
    "gpmodel_svi.kernel.set_prior(\"lengthscale\", dist.LogNormal(0.0, 1.0).expand((2,)).to_event(1))\n",
    "gpmodel_svi.kernel.set_prior(\"variance\", dist.LogNormal(0.0, 1.0))\n",
    "gpmodel_svi.set_prior(\"noise\", dist.LogNormal(0.0, 1.0))\n",
    "\n",
    "# Set guides\n",
    "gpmodel_svi.kernel.autoguide(\"lengthscale\", dist.Normal)\n",
    "gpmodel_svi.kernel.autoguide(\"variance\", dist.Normal)\n",
    "gpmodel_svi.autoguide(\"noise\", dist.Normal)\n",
    "\n",
    "optimizer = torch.optim.Adam(gpmodel_svi.parameters(), lr=0.005)\n",
    "\n",
    "loss_fn = pyro.infer.TraceMeanField_ELBO().differentiable_loss\n",
    "losses = []\n",
    "\n",
    "num_steps = 2000\n",
    "for i in range(num_steps):\n",
    "    optimizer.zero_grad()\n",
    "    loss = loss_fn(gpmodel_svi.model, gpmodel_svi.guide)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss.item())\n",
    "    \n",
    "plt.semilogy(losses);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instead of having specific estimated lengthscales, noises, and variances we now have distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(list(gpmodel_svi.named_buffers()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There's some amount of variation in the GP mean and variance, then:\n",
    "\n",
    "(note: the GP means and variances are all independent samples; left and right plots are unrelated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    plt.figure(figsize=(8,3))\n",
    "    with torch.no_grad():\n",
    "        plt.subplot(121)\n",
    "        plt.title(\"GP mean\")\n",
    "        plot_function(lambda X: gpmodel_svi(X)[0])\n",
    "        plt.subplot(122)\n",
    "        plt.title(\"GP variance\")\n",
    "        plot_function(lambda X: gpmodel_svi(X)[1])\n",
    "        plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametric model in pyro\n",
    "\n",
    "The parametric model used in the BOAT example, with two parameters $\\alpha, \\beta, \\gamma$, is\n",
    "\n",
    "$g(x; \\alpha, \\beta, \\gamma) = \\alpha \\cos(x_1) + \\beta x_1^4 + x_2^2 + \\gamma$\n",
    "\n",
    "with uniform priors on $\\alpha \\in [0, 20]$ and $\\beta \\in [0, 20]$.\n",
    "\n",
    "The $\\gamma$ term isn't actually part of their model, but if I understand the GP code there correctly, they also estimate a mean, and this also has a uniform prior on $[0, 20]$.\n",
    "\n",
    "First, let's just look at what this parametric model looks like, without fitting it to data at all.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parametric_fn(X, alpha, beta, gamma):\n",
    "    x1 = X[...,0]\n",
    "    x2 = X[...,1]\n",
    "    return alpha * torch.cos(x1) + beta*torch.pow(x1, 4) + torch.pow(x2, 2) + gamma\n",
    "\n",
    "def parametric_prior():\n",
    "    alpha = pyro.sample('alpha', dist.Uniform(0, 20))\n",
    "    beta = pyro.sample('beta', dist.Uniform(0, 20))\n",
    "    gamma = pyro.sample('gamma', dist.Uniform(0, 20))\n",
    "    return alpha, beta, gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,3))\n",
    "for i in range(3):\n",
    "    plt.subplot(1,3,i+1)\n",
    "    plot_function(lambda x: parametric_fn(x, *parametric_prior()))\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two things of note:\n",
    "\n",
    "1. This doesn't look like Branin-Hoo at all\n",
    "2. For the main thing that changes for multiple samples is the overall scale, not the shape\n",
    "\n",
    "To fit this model to data, we will want to define a `guide` which samples $\\alpha, \\beta$. This can be as simple as two independent Gaussians (though ideally they would be truncated to $[0, 20]$).\n",
    "\n",
    "We will also need to define a model class which links the parametric model with the data â€” in the semi-parametric model, this will be replaced by a GP, but to see what the parametric model would learn by itself we can instead let the errors be small independent Gaussian noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guide(*args):\n",
    "    mu_a = pyro.param('mu_a', torch.tensor(10.0), constraint=constraints.interval(0, 20))\n",
    "    mu_b = pyro.param('mu_b', torch.tensor(10.0), constraint=constraints.interval(0, 20))\n",
    "    mu_c = pyro.param('mu_c', torch.tensor(10.0), constraint=constraints.interval(0, 20))\n",
    "    sd_a = pyro.param('sd_a', torch.tensor(1.0), constraint=constraints.positive)\n",
    "    sd_b = pyro.param('sd_b', torch.tensor(1.0), constraint=constraints.positive)\n",
    "    sd_c = pyro.param('sd_c', torch.tensor(1.0), constraint=constraints.positive)\n",
    "    alpha = pyro.sample('alpha', dist.Normal(mu_a, sd_a))\n",
    "    beta = pyro.sample('beta', dist.Normal(mu_b, sd_b))\n",
    "    gamma = pyro.sample('gamma', dist.Normal(mu_c, sd_c))\n",
    "    return alpha, beta, gamma\n",
    "\n",
    "def model_parametric(X, y):\n",
    "    g = parametric_fn(X, *parametric_prior())\n",
    "    pyro.sample('f', dist.Normal(g, 0.1).independent(1), obs=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The actual inference code is a little bit different here.\n",
    "\n",
    "We're following the simple example in http://pyro.ai/examples/svi_part_i.html. Main difference is that this uses pyro's optimizers (i.e. `pyro.optim.Adam`), rather than `torch.optim.Adam`; the pyro optimizer does something to automatically extract the relevant parameters, which we explicitly passed to the torch optimizer.\n",
    "\n",
    "(This setup is simpler, but I can't figure out how to make it work with the GP classâ€¦)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.clear_param_store()\n",
    "svi = pyro.infer.SVI(model_parametric, guide, pyro.optim.Adam({'lr': 0.1}), pyro.infer.Trace_ELBO())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "losses = []\n",
    "\n",
    "num_steps = 2000\n",
    "for i in range(num_steps):\n",
    "    losses.append(svi.step(X, y))\n",
    "plt.semilogy(losses);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"alpha ~ Normal(%0.2f, %0.2f)\" % (pyro.param('mu_a'), pyro.param('sd_a')))\n",
    "print(\"beta ~ Normal(%0.2f, %0.2f)\" %  (pyro.param('mu_b'), pyro.param('sd_b')))\n",
    "print(\"gamma ~ Normal(%0.2f, %0.2f)\" %  (pyro.param('mu_c'), pyro.param('sd_c')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Samples from the parametric model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,3))\n",
    "for i in range(3):\n",
    "    plt.subplot(1,3,i+1)\n",
    "    with torch.no_grad():\n",
    "        plot_function(lambda x: parametric_fn(x, *guide()))\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semi-parametric model\n",
    "\n",
    "The semi-parametric model is going to be very close to the parametric model, but will replace the independent normal distribution on the errors with a GP.\n",
    "\n",
    "We'll do this a little bit differently this time; there are enough parameters that they will be slightly annoying to keep track of, and we'll place everything into a single class that extends torch's `nn.Module`.\n",
    "\n",
    "This will duplicate some of the code above, with the advantage of being fully self-contained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.clear_param_store()\n",
    "\n",
    "class SemiParametricModel(nn.Module):\n",
    "\n",
    "    def __init__(self, X, y):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Store data\n",
    "        D = X.shape[-1]\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "        # Define parameters for parametric model\n",
    "        # TODO: I couldn't figure out how to do this using `pyro.param`, so instead\n",
    "        #       I am using `nn.Parameter`. This is annoying, because now constraints\n",
    "        #       need to be handled manually, using the properties below\n",
    "        self._mu_a = nn.Parameter(torch.zeros(1))\n",
    "        self._mu_b = nn.Parameter(torch.zeros(1))\n",
    "        self._mu_c = nn.Parameter(torch.zeros(1))\n",
    "        self._sd_a = nn.Parameter(torch.zeros(1))\n",
    "        self._sd_b = nn.Parameter(torch.zeros(1))\n",
    "        self._sd_c = nn.Parameter(torch.zeros(1))\n",
    "        \n",
    "        self._mu_transform = transform_to(constraints.interval(0, 20))\n",
    "        self._sd_transform = transform_to(constraints.positive)\n",
    "\n",
    "        # Define GP regressor (leave the data arguments empty for now)\n",
    "        self.gp = gp.models.GPRegression(torch.empty((0, D)), torch.empty((0,)),\n",
    "                                         kernel=gp.kernels.Matern52(input_dim=D, lengthscale=torch.ones(D)))\n",
    "\n",
    "        # Set priors for GP (these are the values used in the semiparametric BOAT model, which assumes noiseless GP)\n",
    "        self.gp.kernel.set_prior(\"lengthscale\", dist.LogNormal(0.0, 15.0).expand((2,)).to_event(1))\n",
    "        self.gp.kernel.set_prior(\"variance\", dist.Uniform(0.0, 20.0))\n",
    "        self.gp.set_prior(\"noise\", dist.Uniform(0.0, 1.0))\n",
    "\n",
    "        # Set guides for GP\n",
    "        self.gp.kernel.autoguide(\"lengthscale\", dist.Normal)\n",
    "        self.gp.kernel.autoguide(\"variance\", dist.Normal)\n",
    "        self.gp.autoguide(\"noise\", dist.Normal)\n",
    "    \n",
    "    @property\n",
    "    def mu_a(self): return self._mu_transform(self._mu_a)\n",
    "\n",
    "    @property\n",
    "    def mu_b(self): return self._mu_transform(self._mu_b)\n",
    "\n",
    "    @property\n",
    "    def mu_c(self): return self._mu_transform(self._mu_c)\n",
    "\n",
    "    @property\n",
    "    def sd_a(self): return self._sd_transform(self._sd_a)\n",
    "\n",
    "    @property\n",
    "    def sd_b(self): return self._sd_transform(self._sd_b)\n",
    "    \n",
    "    @property\n",
    "    def sd_c(self): return self._sd_transform(self._sd_c)\n",
    "    \n",
    "    def guide(self):\n",
    "        self.gp.guide()\n",
    "        alpha = pyro.sample('alpha', dist.Normal(self.mu_a, self.sd_a))\n",
    "        beta = pyro.sample('beta', dist.Normal(self.mu_b, self.sd_b))\n",
    "        gamma = pyro.sample('gamma', dist.Normal(self.mu_c, self.sd_c))\n",
    "        return alpha, beta, gamma\n",
    "\n",
    "    def model(self):\n",
    "        alpha = pyro.sample('alpha', dist.Uniform(0, 20))\n",
    "        beta = pyro.sample('beta', dist.Uniform(0, 20))\n",
    "        gamma = pyro.sample('gamma', dist.Uniform(0, 20))\n",
    "        g = parametric_fn(self.X, alpha, beta, gamma)\n",
    "        residual = self.y - g\n",
    "        # update the GP to now model the residual from the parametric model\n",
    "        self.gp.set_data(self.X, residual)\n",
    "        # call GP model function to actually make the observation\n",
    "        self.gp.model()\n",
    "        \n",
    "    def forward(self, X):\n",
    "        g = parametric_fn(X, *self.guide())\n",
    "        mu, sigma = self.gp(X)\n",
    "        return g + mu, sigma\n",
    "\n",
    "semi_parametric = SemiParametricModel(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It might be helpful to use different learning rates for the different sets of parameters...\n",
    "\n",
    "When I tried earlier without this, results looked strange."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_opt = torch.optim.Adam(semi_parametric.parameters(recurse=False), lr=0.1)\n",
    "gp_opt = torch.optim.Adam(semi_parametric.gp.parameters(), lr=0.005)\n",
    "loss_fn = pyro.infer.Trace_ELBO().differentiable_loss\n",
    "\n",
    "losses = []\n",
    "\n",
    "num_steps = 2000\n",
    "for i in range(num_steps):\n",
    "    gp_opt.zero_grad()\n",
    "    param_opt.zero_grad()\n",
    "    loss = loss_fn(semi_parametric.model, semi_parametric.guide)\n",
    "    loss.backward()\n",
    "    gp_opt.step()\n",
    "    param_opt.step()\n",
    "    losses.append(loss.item())\n",
    "\n",
    "plt.semilogy(losses);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"alpha ~ Normal(%0.2f, %0.2f)\" % (semi_parametric.mu_a.item(), semi_parametric.sd_a.item()))\n",
    "print(\"beta ~ Normal(%0.2f, %0.2f)\" % (semi_parametric.mu_b.item(), semi_parametric.sd_b.item()))\n",
    "print(\"gamma ~ Normal(%0.2f, %0.2f)\" % (semi_parametric.mu_c.item(), semi_parametric.sd_c.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(list(semi_parametric.gp.named_buffers()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Samples of the mean function from the semi-parametric model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    plt.figure(figsize=(8,3))\n",
    "    with torch.no_grad():\n",
    "        plt.subplot(121)\n",
    "        plt.title(\"GP mean\")\n",
    "        plot_function(lambda X: semi_parametric(X)[0])\n",
    "        plt.subplot(122)\n",
    "        plt.title(\"GP mean\")\n",
    "        plot_function(lambda X: semi_parametric(X)[0])\n",
    "        plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As a reminder, here's the real Branin-Hoo function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "plot_function(branin_hoo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
