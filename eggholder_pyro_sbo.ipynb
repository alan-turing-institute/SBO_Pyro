{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured Bayesian Optimisation with Pyro (2D Eggholder function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pyro\n",
    "import math\n",
    "import time\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "\n",
    "from torch.distributions import constraints, transform_to\n",
    "\n",
    "import pyro.distributions as dist\n",
    "import pyro.contrib.gp as gp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "seed_number = 555"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed_number)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)\n",
    "print(pyro.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_function(f, x1_min, x1_max, x2_min, x2_max, n_points=100, ticks=9):\n",
    "    XX, YY = np.meshgrid(np.linspace(x1_min, x1_max, n_points), np.linspace(x2_min, x2_max, n_points))\n",
    "    ZZ = f(torch.FloatTensor(np.stack([XX.ravel(), YY.ravel()]).T))\n",
    "    plt.imshow(ZZ.reshape(n_points, n_points))\n",
    "    plt.xticks(np.linspace(0, n_points, ticks), np.linspace(x1_min, x1_max, ticks))\n",
    "    plt.yticks(np.linspace(0, n_points, ticks), np.linspace(x2_min, x2_max, ticks)) \n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    plt.set_cmap('jet')\n",
    "    plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the eggholder function as a _challenging_ example (https://www.sfu.ca/~ssurjano/egg.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eggholder(x):\n",
    "    x1 = x[...,0]\n",
    "    x2 = x[...,1]\n",
    "    \n",
    "    return -((x2 + 47) * torch.sin(torch.sqrt(torch.abs(x2 + x1/2 + 47))) - \n",
    "             x1 * torch.sin(torch.sqrt(torch.abs(x1 - x2 + 47))))\n",
    "\n",
    "# Global minimum at 512, 404.2319 with value of -959.6407"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial (training) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_points = 200\n",
    "\n",
    "X = torch.rand(N_points, 2)*1024 + torch.FloatTensor([-512, -512])\n",
    "\n",
    "y = eggholder(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const_x1_min = -512\n",
    "const_x2_min = -512\n",
    "const_x1_max = 512\n",
    "const_x2_max = 512\n",
    "\n",
    "steps = 1000\n",
    "strides = 100\n",
    "\n",
    "X1 = torch.linspace(const_x1_min, const_x1_max, steps)\n",
    "X2 = torch.linspace(const_x2_min, const_x2_max, steps)\n",
    "\n",
    "X1_mesh, X2_mesh = torch.meshgrid(X1, X2)\n",
    "\n",
    "Z_mesh = eggholder(torch.stack((X1_mesh, X2_mesh), dim=2))\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.contour3D(X1_mesh, X2_mesh, Z_mesh, 50)\n",
    "ax.set_xlabel('x1')\n",
    "ax.set_ylabel('x2')\n",
    "ax.set_zlabel('y');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.contour(X1_mesh, X2_mesh, Z_mesh, strides);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parametric model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parametric_fn(X, alpha, beta, gamma, delta):\n",
    "    x1 = X[...,0]\n",
    "    x2 = X[...,1]\n",
    "    \n",
    "#     return (gamma*torch.sqrt(torch.abs(x1))*torch.cos(alpha*torch.sqrt(torch.abs(x1 + x2)))\n",
    "#             + delta*torch.sqrt(torch.abs(x2))*torch.sin(beta*torch.sqrt(torch.abs(x1 - x2))))\n",
    "    \n",
    "    \n",
    "    return -((x2*alpha )*torch.cos(torch.sqrt(torch.abs(x2 + x1 - gamma)))\n",
    "            + (x1*beta )*torch.cos(torch.sqrt(torch.abs(x2 - x1 - delta))))\n",
    "  \n",
    "    \n",
    "#    return (x1*torch.cos(2*alpha*math.pi*x1) + x2*torch.cos(2*beta*math.pi*x2))\n",
    "\n",
    "def parametric_prior():\n",
    "    \n",
    "    alpha = pyro.sample('alpha', dist.Uniform(-10, 10))\n",
    "    beta = pyro.sample('beta', dist.Uniform(-10, 10))\n",
    "    gamma = pyro.sample('gamma', dist.Uniform(-10, 10))\n",
    "    delta = pyro.sample('delta', dist.Uniform(-10, 20))\n",
    "    \n",
    "    return alpha, beta, gamma, delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising parametric function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,3))\n",
    "for i in range(3):\n",
    "    plt.subplot(1,3,i+1)\n",
    "    plot_function(lambda x: parametric_fn(x, *parametric_prior()), \n",
    "                  const_x1_min, const_x1_max, const_x2_min, const_x2_max)\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference of the parametric model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guide(*args):\n",
    "    mu_a = pyro.param('mu_a', torch.tensor(0.5), constraint=constraints.interval(-10, 10))\n",
    "    mu_b = pyro.param('mu_b', torch.tensor(0.5), constraint=constraints.interval(-10, 10))\n",
    "    mu_c = pyro.param('mu_c', torch.tensor(1.0), constraint=constraints.interval(-10, 10))\n",
    "    mu_d = pyro.param('mu_d', torch.tensor(1.0), constraint=constraints.interval(-10, 20))\n",
    "    \n",
    "    sd_a = pyro.param('sd_a', torch.tensor(0.2), constraint=constraints.positive)\n",
    "    sd_b = pyro.param('sd_b', torch.tensor(0.2), constraint=constraints.positive)\n",
    "    sd_c = pyro.param('sd_c', torch.tensor(1.0), constraint=constraints.positive)\n",
    "    sd_d = pyro.param('sd_d', torch.tensor(1.0), constraint=constraints.positive)\n",
    "\n",
    "    alpha = pyro.sample('alpha', dist.Normal(mu_a, sd_a))\n",
    "    beta = pyro.sample('beta', dist.Normal(mu_b, sd_b))\n",
    "    gamma = pyro.sample('gamma', dist.Normal(mu_c, sd_c))\n",
    "    delta = pyro.sample('delta', dist.Normal(mu_d, sd_d))\n",
    "    \n",
    "    return alpha, beta, gamma, delta\n",
    "\n",
    "def model_parametric(X, y):\n",
    "    g = parametric_fn(X, *parametric_prior())\n",
    "    pyro.sample('f', dist.Normal(g, 0.1).independent(1), obs=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.clear_param_store()\n",
    "svi = pyro.infer.SVI(model_parametric, guide, pyro.optim.Adam({'lr': 0.005}), pyro.infer.Trace_ELBO())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "losses = []\n",
    "num_steps = 2000\n",
    "for i in range(num_steps):\n",
    "    losses.append(svi.step(X, y))\n",
    "\n",
    "plt.semilogy(losses);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"alpha ~ Normal(%0.2f, %0.2f)\" % (pyro.param('mu_a'), pyro.param('sd_a')))\n",
    "print(\"beta ~ Normal(%0.2f, %0.2f)\" %  (pyro.param('mu_b'), pyro.param('sd_b')))\n",
    "print(\"gamma ~ Normal(%0.4f, %0.4f)\" %  (pyro.param('mu_c'), pyro.param('sd_c')))\n",
    "print(\"delta ~ Normal(%0.4f, %0.4f)\" %  (pyro.param('mu_d'), pyro.param('sd_d')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    plt.figure(figsize=(12,3))\n",
    "    for i in range(3):\n",
    "        plt.subplot(1,3,i+1)\n",
    "        plot_function(lambda x: parametric_fn(x, *guide()), \n",
    "                      const_x1_min, const_x1_max, const_x2_min, const_x2_max)\n",
    "    plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semi-parametric model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.clear_param_store()\n",
    "\n",
    "class SemiParametricModel(nn.Module):\n",
    "\n",
    "    def __init__(self, X, y):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Store data\n",
    "        D = X.shape[-1]\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "        # Define parameters for parametric model\n",
    "        # TODO: I couldn't figure out how to do this using `pyro.param`, so instead\n",
    "        #       I am using `nn.Parameter`. This is annoying, because now constraints\n",
    "        #       need to be handled manually, using the properties below\n",
    "        self._mu_a = nn.Parameter(torch.zeros(1))\n",
    "        self._mu_b = nn.Parameter(torch.zeros(1))\n",
    "        self._mu_c = nn.Parameter(torch.zeros(1))\n",
    "        self._mu_d = nn.Parameter(torch.zeros(1))\n",
    "        \n",
    "        self._sd_a = nn.Parameter(torch.zeros(1))\n",
    "        self._sd_b = nn.Parameter(torch.zeros(1))\n",
    "        self._sd_c = nn.Parameter(torch.zeros(1))\n",
    "        self._sd_d = nn.Parameter(torch.zeros(1))\n",
    "        \n",
    "        self._mu_transform = transform_to(constraints.interval(-10, 20))\n",
    "        self._sd_transform = transform_to(constraints.positive)\n",
    "\n",
    "        # Define GP regressor (leave the data arguments empty for now)\n",
    "        self.gp = gp.models.GPRegression(torch.empty((0, D)), torch.empty((0,)),\n",
    "                                         kernel=gp.kernels.Matern52(input_dim=D, lengthscale=torch.ones(D)))\n",
    "\n",
    "        #self.gp.kernel.set_prior(\"lengthscale\", dist.Uniform(-100, 100.0).expand((2,)).to_event(1))\n",
    "        #self.gp.kernel.set_prior(\"variance\", dist.Uniform(0, 100.0))\n",
    "        \n",
    "        # Set priors for GP (these are the values used in the semiparametric BOAT model, which assumes noiseless GP)\n",
    "        self.gp.kernel.set_prior(\"lengthscale\", dist.LogNormal(1.0, 100.0).expand((2,)).to_event(1))\n",
    "        self.gp.kernel.set_prior(\"variance\", dist.Uniform(0.0, 10.0))\n",
    "        self.gp.set_prior(\"noise\", dist.Uniform(0.0, 1.0))\n",
    "\n",
    "        # Set guides for GP\n",
    "        self.gp.kernel.autoguide(\"lengthscale\", dist.Normal)\n",
    "        self.gp.kernel.autoguide(\"variance\", dist.Normal)\n",
    "        self.gp.autoguide(\"noise\", dist.Normal)\n",
    "    \n",
    "    @property\n",
    "    def mu_a(self): return self._mu_transform(self._mu_a)\n",
    "\n",
    "    @property\n",
    "    def mu_b(self): return self._mu_transform(self._mu_b)\n",
    "    \n",
    "    @property\n",
    "    def mu_c(self): return self._mu_transform(self._mu_c)\n",
    "    \n",
    "    @property\n",
    "    def mu_d(self): return self._mu_transform(self._mu_d)\n",
    "\n",
    "    @property\n",
    "    def sd_a(self): return self._sd_transform(self._sd_a)\n",
    "\n",
    "    @property\n",
    "    def sd_b(self): return self._sd_transform(self._sd_b)\n",
    "    \n",
    "    @property\n",
    "    def sd_c(self): return self._sd_transform(self._sd_c)\n",
    "    \n",
    "    @property\n",
    "    def sd_d(self): return self._sd_transform(self._sd_d)\n",
    "    \n",
    "    def guide(self):\n",
    "        self.gp.guide()\n",
    "        \n",
    "        alpha = pyro.sample('alpha', dist.Normal(self.mu_a, self.sd_a))\n",
    "        beta = pyro.sample('beta', dist.Normal(self.mu_b, self.sd_b))\n",
    "        gamma = pyro.sample('gamma', dist.Normal(self.mu_c, self.sd_c))\n",
    "        delta = pyro.sample('delta', dist.Normal(self.mu_d, self.sd_d))\n",
    "        \n",
    "        return alpha, beta, gamma, delta\n",
    "\n",
    "    def model(self):\n",
    "       \n",
    "        alpha = pyro.sample('alpha', dist.Uniform(-10, 10))\n",
    "        beta = pyro.sample('beta', dist.Uniform(-10, 10))\n",
    "        gamma = pyro.sample('gamma', dist.Uniform(-10, 10))\n",
    "        delta = pyro.sample('delta', dist.Uniform(-10, 20))\n",
    "        \n",
    "        g = parametric_fn(self.X, alpha, beta, gamma, delta)\n",
    "        \n",
    "        residual = self.y - g\n",
    "        \n",
    "        # update the GP to now model the residual from the parametric model\n",
    "        self.gp.set_data(self.X, residual)\n",
    "                \n",
    "        # call GP model function to actually make the observation\n",
    "        self.gp.model()\n",
    " \n",
    "    def forward(self, X):\n",
    "        g = parametric_fn(X, *self.guide())\n",
    "        mu, sigma = self.gp(X)        \n",
    "        return g + mu, sigma\n",
    "\n",
    "semi_parametric = SemiParametricModel(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pyro.get_param_store()\n",
    "result.get_all_param_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(semi_parametric.parameters(recurse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_opt = torch.optim.Adam(semi_parametric.parameters(recurse=False), lr=0.1)\n",
    "gp_opt = torch.optim.Adam(semi_parametric.gp.parameters(), lr=0.005)\n",
    "loss_fn = pyro.infer.Trace_ELBO().differentiable_loss\n",
    "\n",
    "losses = []\n",
    "\n",
    "num_steps = 5000\n",
    "for i in range(num_steps):\n",
    "    gp_opt.zero_grad()\n",
    "    param_opt.zero_grad()\n",
    "    loss = loss_fn(semi_parametric.model, semi_parametric.guide)\n",
    "    loss.backward()\n",
    "    gp_opt.step()\n",
    "    param_opt.step()\n",
    "    losses.append(loss.item())\n",
    "\n",
    "plt.semilogy(losses);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"alpha ~ Normal(%0.2f, %0.2f)\" % (semi_parametric.mu_a.item(), semi_parametric.sd_a.item()))\n",
    "print(\"beta ~ Normal(%0.2f, %0.2f)\" % (semi_parametric.mu_b.item(), semi_parametric.sd_b.item()))\n",
    "print(\"gamma ~ Normal(%0.4f, %0.4f)\" % (semi_parametric.mu_c.item(), semi_parametric.sd_c.item()))\n",
    "print(\"delta ~ Normal(%0.4f, %0.4f)\" % (semi_parametric.mu_d.item(), semi_parametric.sd_d.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(list(semi_parametric.gp.named_buffers()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    plt.figure(figsize=(8, 3))\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        plt.subplot(121)\n",
    "        plt.title(\"GP mean\")\n",
    "        plot_function(lambda X: semi_parametric(X)[0], const_x1_min, const_x1_max, const_x2_min, const_x2_max)\n",
    "        \n",
    "        plt.subplot(122)\n",
    "        plt.title(\"GP mean\")\n",
    "        plot_function(lambda X: semi_parametric(X)[0], const_x1_min, const_x1_max, const_x2_min, const_x2_max)\n",
    "        plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.contour(X1_mesh, X2_mesh, Z_mesh, strides);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acquisition function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_phi = lambda x: torch.exp(-x.pow(2)/2)/np.sqrt(2*np.pi)\n",
    "normal_Phi = lambda x: (1 + torch.erf(x / np.sqrt(2))) / 2\n",
    "\n",
    "def expected_improvement(x):\n",
    "    \n",
    "    y_min = semi_parametric.gp.y.min()\n",
    "    \n",
    "    mu, variance = semi_parametric(x)\n",
    "    #semi_parametric.gp(x, full_cov=False, noiseless=False)\n",
    "    \n",
    "    sigma = variance.sqrt()\n",
    "    \n",
    "    delta = y_min - mu\n",
    "    \n",
    "    EI = delta.clamp_min(0.0) + sigma*normal_phi(delta/sigma) - delta.abs()*normal_Phi(delta/sigma)\n",
    "    \n",
    "    return -EI\n",
    "\n",
    "def acquisition_func(x):\n",
    " \n",
    "    return expected_improvement(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_steps=1000):\n",
    "    \n",
    "    param_opt = torch.optim.Adam(semi_parametric.parameters(recurse=False), lr=0.1)\n",
    "    gp_opt = torch.optim.Adam(semi_parametric.gp.parameters(), lr=0.005)\n",
    "    \n",
    "    loss_fn = pyro.infer.Trace_ELBO().differentiable_loss\n",
    "    \n",
    "    losses = []\n",
    "        \n",
    "    for i in range(num_steps):\n",
    "        gp_opt.zero_grad()\n",
    "        param_opt.zero_grad()\n",
    "        loss = loss_fn(semi_parametric.model, semi_parametric.guide)\n",
    "        loss.backward()\n",
    "        gp_opt.step()\n",
    "        param_opt.step()\n",
    "        losses.append(loss.item())\n",
    "    \n",
    "    return losses\n",
    "\n",
    "def find_a_candidate(x_init):\n",
    "    \n",
    "    # Creating constrains\n",
    "    constraint_x1 = constraints.interval(const_x1_min, const_x1_max)\n",
    "    constraint_x2 = constraints.interval(const_x2_min, const_x2_max)\n",
    "    \n",
    "    # transform x_init to an unconstrained domain as we use an unconstrained optimizer\n",
    "    unconstrained_x1_init = transform_to(constraint_x1).inv(x_init[:, 0])\n",
    "    unconstrained_x2_init = transform_to(constraint_x2).inv(x_init[:, 1])\n",
    "    x_uncon_init = torch.stack((unconstrained_x1_init, unconstrained_x2_init), dim=1)\n",
    "    \n",
    "    x_uncon = x_uncon_init.clone().detach().requires_grad_(True)\n",
    "    \n",
    "    # unconstrained minimiser\n",
    "    minimizer = optim.LBFGS([x_uncon])\n",
    "\n",
    "    def closure():\n",
    "        minimizer.zero_grad()\n",
    "                \n",
    "        x1_tmp = transform_to(constraint_x1)(x_uncon[:, 0])\n",
    "        x2_tmp = transform_to(constraint_x2)(x_uncon[:, 1])\n",
    "        x = torch.stack((x1_tmp, x2_tmp), dim=1)\n",
    "        \n",
    "        y = acquisition_func(x)\n",
    "        \n",
    "        autograd.backward(x_uncon, autograd.grad(y, x_uncon))\n",
    "                \n",
    "        return y\n",
    "    \n",
    "    minimizer.step(closure)\n",
    "   \n",
    "    # after finding a candidate in the unconstrained domain,\n",
    "    # convert it back to original domain.\n",
    "    x1_tmp = transform_to(constraint_x1)(x_uncon[:, 0])\n",
    "    x2_tmp = transform_to(constraint_x2)(x_uncon[:, 1])\n",
    "    \n",
    "    x = torch.stack((x1_tmp, x2_tmp), dim=1)\n",
    "    \n",
    "    return x.detach()\n",
    "  \n",
    "def next_x(num_candidates=5):\n",
    "    \n",
    "    candidates = []\n",
    "    values = []\n",
    "    \n",
    "    # take the last point as the first attempt\n",
    "    x_init = semi_parametric.X[-1:]\n",
    "    \n",
    "    for i in range(num_candidates):\n",
    "        \n",
    "        x = find_a_candidate(x_init)\n",
    "        y = acquisition_func(x)\n",
    "    \n",
    "        candidates.append(x)\n",
    "        values.append(y)\n",
    "        \n",
    "        # a new random attempt initial point\n",
    "        x_init = torch.stack((\n",
    "                x[:,0].new_empty(1).uniform_(const_x1_min, const_x1_max),\n",
    "                x[:,1].new_empty(1).uniform_(const_x2_min, const_x2_max)), dim=1)\n",
    "        \n",
    "    argmin = torch.min(torch.cat(values), dim=0)[1].item()\n",
    "        \n",
    "    return candidates[argmin]\n",
    "\n",
    "def update_posterior(x_new, viz_flag=False):\n",
    "    \n",
    "    # evaluate f at new point\n",
    "    bh_y = eggholder(x_new) \n",
    "        \n",
    "    # incorporate new evaluation\n",
    "    semi_parametric.X = torch.cat([semi_parametric.X, x_new]) \n",
    "    semi_parametric.y = torch.cat([semi_parametric.y, bh_y])\n",
    "    \n",
    "    losses = train()\n",
    "    \n",
    "    if viz_flag:\n",
    "        plot_model()\n",
    "        \n",
    "def plot_model():\n",
    "    plt.figure(figsize=(12,3)) \n",
    "    \n",
    "    # Acquisition function\n",
    "    plt.subplot(1,4,1)\n",
    "    plt.title(\"Acquisition\")\n",
    "    with torch.no_grad(): \n",
    "        plot_function(acquisition_func, const_x1_min, const_x1_max, const_x2_min, const_x2_max)\n",
    "\n",
    "    # Losses\n",
    "    plt.subplot(1,4,2)\n",
    "    plt.title(\"Losses\")\n",
    "    plt.semilogy(losses);\n",
    "\n",
    "    # Semi-param model mu\n",
    "    plt.subplot(1,4,3)\n",
    "    plt.title(\"Semi-param model $\\mu$\")\n",
    "    with torch.no_grad(): \n",
    "        plot_function(lambda X: semi_parametric(X)[0], const_x1_min, const_x1_max, const_x2_min, const_x2_max)\n",
    "\n",
    "    # Semi-param model sigma\n",
    "    plt.subplot(1,4,4)\n",
    "    plt.title(\"Semi-param model $\\sigma$\")\n",
    "    with torch.no_grad(): \n",
    "        plot_function(lambda X: semi_parametric(X)[1], const_x1_min, const_x1_max, const_x2_min, const_x2_max)\n",
    "\n",
    "    plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# flag to visualise steps\n",
    "viz_flag = True\n",
    "\n",
    "# Restarting the model\n",
    "#pyro.clear_param_store()\n",
    "\n",
    "#semi_parametric = SemiParametricModel(X, y)\n",
    "\n",
    "time_st = time.time()\n",
    "\n",
    "losses = train()\n",
    "\n",
    "if viz_flag:\n",
    "    plot_model()\n",
    "\n",
    "sbo_steps = 2\n",
    "\n",
    "for i in range(sbo_steps):\n",
    "    \n",
    "    xmin = next_x()\n",
    "    \n",
    "    print(\"Step SBO: \", i+1, \"new point: \", xmin)\n",
    "    \n",
    "    update_posterior(xmin, viz_flag)\n",
    "    \n",
    "print(\"Time: \", time.time() - time_st)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
