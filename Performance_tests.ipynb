{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured Bayesian Optimisation with Pyro\n",
    "## Testing performance\n",
    "\n",
    "Goal: compare performances of standard BO-GP, random GP and SBO-GP strategies to minimize Branin-Hoo and hyperbolic functions with pyro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import constraints, transform_to\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import pyro.contrib.gp as gp\n",
    "from pyro.nn import PyroSample, PyroModule\n",
    "from pyro.infer import autoguide, SVI, Trace_ELBO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.0.post2\n",
      "1.0.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(pyro.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimalistic (S)BO approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Acquisition function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_confidence_bound(model, x, kappa=2):\n",
    "    \"\"\" Lower Confidence Bound (LCB): $\\alpha(x)=\\mu(x) - \\kappa\\sigma(x)$ \"\"\"\n",
    "    \n",
    "    mu, variance = model(x)\n",
    "    sigma = variance.sqrt()\n",
    "    \n",
    "    return mu - kappa * sigma\n",
    "\n",
    "normal_phi = lambda x: torch.exp(-x.pow(2)/2)/np.sqrt(2*np.pi)\n",
    "normal_Phi = lambda x: (1 + torch.erf(x / np.sqrt(2))) / 2\n",
    "\n",
    "def expected_improvement(model, x):\n",
    "    \"\"\" Brooks' implementation of expected improvement (EI). \"\"\"\n",
    "    \n",
    "    y_min = model.y.min()\n",
    "    mu, variance = model(x)\n",
    "    sigma = variance.sqrt()\n",
    "    delta = y_min - mu\n",
    "    EI = delta.clamp_min(0.0) + sigma*normal_phi(delta/sigma) - delta.abs()*normal_Phi(delta/sigma)\n",
    "    \n",
    "    return -EI\n",
    "\n",
    "def acquisition_func(model, x, af='EI'):\n",
    "    \"\"\" Defines acquisition function. \"\"\"\n",
    "\n",
    "    if af == \"EI\":\n",
    "        return expected_improvement(model, x)\n",
    "    elif af == \"LCB\":\n",
    "        return lower_confidence_bound(model, x)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, num_steps=1000, adam_params={\"lr\":0.1}):\n",
    "    \"\"\" Trains the semi-parametric model. \"\"\"\n",
    "    \n",
    "    # TODO: check if param store needs to be cleared here.\n",
    "    pyro.clear_param_store()\n",
    "    # setup the inference algorithm\n",
    "    guide = autoguide.AutoMultivariateNormal(model.model)\n",
    "    optimizer = pyro.optim.Adam(adam_params)\n",
    "    loss = Trace_ELBO()\n",
    "    svi = SVI(model.model, guide, optimizer, loss)\n",
    "    # do gradient steps\n",
    "    losses = []\n",
    "    for _ in range(num_steps):\n",
    "        losses.append(svi.step())\n",
    "    \n",
    "    return losses, guide\n",
    "\n",
    "def train_gp(model, num_steps=1000, adam_params={\"lr\":0.1}):\n",
    "    \"\"\" Trains the gp model. \"\"\"\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=adam_params['lr'])\n",
    "    loss_fn = pyro.infer.TraceMeanField_ELBO().differentiable_loss\n",
    "    losses = gp.util.train(model, optimizer, loss_fn, num_steps=num_steps)\n",
    "\n",
    "    return losses, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding new candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_a_candidate(model, x_init, constr, num_steps=1000, lr=0.1):\n",
    "    \"\"\" Finds new candidate \"\"\"\n",
    "    \n",
    "    def transf_values(values, constr, dims, inv_mode=False):\n",
    "        \"\"\" Transforming (un)constrained variables to (un)constrained domain \"\"\"\n",
    "        \n",
    "        x_tmp = ()\n",
    "        for i in range(dims):\n",
    "            if inv_mode:\n",
    "                x_tmp += (transform_to(constr[i]).inv(values[:, i]), )\n",
    "            else:\n",
    "                x_tmp += (transform_to(constr[i])(values[:, i]), )\n",
    "            \n",
    "        x = torch.stack(x_tmp, dim=1)\n",
    "        return x\n",
    "            \n",
    "    x_dims = x_init.shape[-1]\n",
    "    \n",
    "    x_uncon_init = transf_values(x_init, constr, x_dims, inv_mode=True)\n",
    "    x_uncon = x_uncon_init.clone().detach().requires_grad_(True)\n",
    "    \n",
    "    # unconstrained minimiser \n",
    "    # TODO: at the moment we are using torch optimizer, should we change to pyro?\n",
    "    \n",
    "    minimizer = optim.Adam([x_uncon], lr=lr)\n",
    "    \n",
    "    #minimizer = optim.LBFGS([x_uncon], line_search_fn='strong_wolfe')\n",
    "    \n",
    "    def closure():\n",
    "        minimizer.zero_grad()\n",
    "        x = transf_values(x_uncon, constr, x_dims)\n",
    "        y = acquisition_func(model, x)\n",
    "        autograd.backward(x_uncon, autograd.grad(y, x_uncon))      \n",
    "        return y\n",
    "    \n",
    "    for _ in range(num_steps):\n",
    "        minimizer.step(closure)\n",
    "   \n",
    "    x = transf_values(x_uncon, constr, x_dims)\n",
    "    \n",
    "    return x.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_x(model, constr, num_candidates=5, num_steps=1000, lr=0.1):\n",
    "    \"\"\" Finds the next best candidate on the acquisition function surface \"\"\"\n",
    "    \n",
    "    candidates = []\n",
    "    values = []\n",
    "    \n",
    "    # start with last step\n",
    "    x_init = model.X[-1:]\n",
    "    for i in range(num_candidates):\n",
    "\n",
    "        x = find_a_candidate(model, x_init, constr, num_steps=num_steps, lr=lr)\n",
    "        y = acquisition_func(model, x)\n",
    "    \n",
    "        candidates.append(x)\n",
    "        values.append(y)\n",
    "        \n",
    "        # a new random attempt initial point\n",
    "        x_init = torch.stack((\n",
    "                x[:,0].new_empty(1).uniform_(const_x1_min, const_x1_max),\n",
    "                x[:,1].new_empty(1).uniform_(const_x2_min, const_x2_max)), dim=1)\n",
    "        \n",
    "    argmin = torch.min(torch.cat(values), dim=0)[1].item()\n",
    "        \n",
    "    return candidates[argmin]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Updating posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_posterior(model, obj_function, x_new, num_steps=1000, adam_params={\"lr\":0.1}, \n",
    "                    gp_mode=False):\n",
    "    \n",
    "    # evaluate f at new point\n",
    "    bh_y = obj_function(x_new) \n",
    "        \n",
    "    # incorporate new evaluation\n",
    "    model.X = torch.cat([model.X, x_new]) \n",
    "    model.y = torch.cat([model.y, bh_y])\n",
    "    \n",
    "    if not gp_mode:\n",
    "        losses, guide = train(model, num_steps=num_steps, adam_params=adam_params)\n",
    "    else:\n",
    "        losses, guide = train_gp(model, num_steps=num_steps, adam_params=adam_params)\n",
    "    \n",
    "    return guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_obj(obj_function):\n",
    "\n",
    "    steps = 1000\n",
    "    strides = 200\n",
    "\n",
    "    X1 = torch.linspace(const_x1_min, const_x1_max, steps)\n",
    "    X2 = torch.linspace(const_x2_min, const_x2_max, steps)\n",
    "\n",
    "    X1_mesh, X2_mesh = torch.meshgrid(X1, X2)\n",
    "    \n",
    "    Z_mesh = obj_function(torch.stack((X1_mesh.flatten(), X2_mesh.flatten()), dim=1)).reshape(steps, steps)\n",
    "    plt.contour(\n",
    "        X1_mesh.detach().numpy(), \n",
    "        X2_mesh.detach().numpy(), \n",
    "        Z_mesh.detach().numpy(), strides)\n",
    "    \n",
    "    plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_solution(xmins, target_lms):\n",
    "    closest_dist = np.inf\n",
    "    closest_point = None\n",
    "    \n",
    "    for xmin in xmins:\n",
    "        for bh_lm in target_lms:\n",
    "            dist = np.linalg.norm(xmin-bh_lm)\n",
    "            if dist < closest_dist: \n",
    "                closest_dist = dist\n",
    "                closest_point = xmin\n",
    "                \n",
    "    return closest_point, closest_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Branin-Hoo example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const_x1_min = -5\n",
    "const_x1_max = 10\n",
    "\n",
    "const_x2_min = 0\n",
    "const_x2_max = 15\n",
    "\n",
    "# Creating constraints\n",
    "constr = [\n",
    "    constraints.interval(const_x1_min, const_x1_max),\n",
    "    constraints.interval(const_x2_min, const_x2_max)\n",
    "]\n",
    "\n",
    "def branin_hoo(x):\n",
    "    \"\"\" Compute Branin-Hoo function for fixed constants \"\"\"\n",
    "    a = 1.0\n",
    "    b = 5.1 / (4 * np.pi**2)\n",
    "    c = 5.0 / np.pi\n",
    "    r = 6.0\n",
    "    s = 10.0\n",
    "    t = 1.0 / (8 * np.pi)\n",
    "    x1 = x[...,0]\n",
    "    x2 = x[...,1]\n",
    "    return a * (x2 - b*x1**2 + c*x1 - r)**2 + s*(1 - t)*torch.cos(x1) + s\n",
    "\n",
    "\n",
    "branin_hoo_lms_np = np.stack(\n",
    "    (np.array([-math.pi, math.pi, 9.42478]), \n",
    "     np.array([12.275, 2.275, 2.475])), axis=1)\n",
    "\n",
    "# Checking LMs\n",
    "branin_hoo_lms = torch.stack(\n",
    "    (torch.tensor([-math.pi, math.pi, 9.42478]), \n",
    "     torch.tensor([12.275, 2.275, 2.475])), dim=1)\n",
    "\n",
    "assert np.allclose(branin_hoo(branin_hoo_lms).numpy(),\n",
    "        np.array([0.397887, 0.397887, 0.397887], dtype=np.float32), rtol=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_obj(branin_hoo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_random_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training points\n",
    "N_train = 10\n",
    "X_train = torch.rand(N_train, 2)*15 + torch.FloatTensor([const_x1_min, const_x2_min])\n",
    "y_train = branin_hoo(X_train)\n",
    "\n",
    "# Optimiser parameters\n",
    "adam_num_steps = 1000\n",
    "adam_params={\"lr\": 0.1}\n",
    "\n",
    "num_candidates = 10\n",
    "\n",
    "bo_steps = 20\n",
    "\n",
    "num_tests = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BO approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bo_tests_search_points = []\n",
    "\n",
    "for test_i in range(num_tests):\n",
    "    try:\n",
    "        print(\"TEST: \", test_i+1)\n",
    "\n",
    "        pyro.clear_param_store()\n",
    "\n",
    "        gp_model_svi = gp.models.GPRegression(X_train, y_train, \n",
    "                                              gp.kernels.Matern52(input_dim=X_train.shape[1], \n",
    "                                                                  lengthscale=100*torch.ones(X_train.shape[1])))\n",
    "\n",
    "        # Set priors\n",
    "        gp_model_svi.kernel.lengthscale = pyro.nn.PyroSample(dist.LogNormal(3, 1).expand([2]).to_event())\n",
    "        gp_model_svi.kernel.variance = pyro.nn.PyroSample(dist.LogNormal(5, 2))\n",
    "        gp_model_svi.noise = pyro.nn.PyroSample(dist.LogNormal(0, 1))\n",
    "\n",
    "        # Set guides\n",
    "        gp_model_svi.kernel.autoguide(\"lengthscale\", dist.Normal)\n",
    "        gp_model_svi.kernel.autoguide(\"variance\", dist.Normal)\n",
    "        gp_model_svi.autoguide(\"noise\", dist.Normal)\n",
    "\n",
    "        # optimise\n",
    "        losses, _ = train_gp(gp_model_svi, num_steps=adam_num_steps, adam_params=adam_params)\n",
    "\n",
    "        xmins = np.zeros([bo_steps, 2], np.float32)\n",
    "\n",
    "        for i in range(bo_steps):\n",
    "            xmin = next_x(gp_model_svi, constr, num_candidates=num_candidates, num_steps=adam_num_steps)\n",
    "\n",
    "            update_posterior(gp_model_svi, branin_hoo, xmin, \n",
    "                             num_steps=adam_num_steps, adam_params=adam_params, gp_mode=True)\n",
    "\n",
    "            xmins[i] = xmin.detach().numpy()\n",
    "\n",
    "            closest_point, closest_dist = find_best_solution([xmins[i]], branin_hoo_lms_np)\n",
    "            print(\"  BO STEP: \", i+1, \"xmin:\", xmins[i], \" distance: \", closest_dist)\n",
    "\n",
    "        # saving results for the run\n",
    "        np.savetxt(\"pyro_results/pyro_bo_%d.out\" % (test_i), xmins, delimiter=',')\n",
    "\n",
    "        closest_point, closest_dist = find_best_solution(xmins, branin_hoo_lms_np)\n",
    "        print(\"  Best candidate: \", closest_point, \" distance: \", closest_dist)\n",
    "\n",
    "        bo_tests_search_points.append(xmins)\n",
    "    except:\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
